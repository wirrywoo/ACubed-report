\section{Introduction and Motivation}
\label{sec:introduction}

\href{https://www.Flash Flash Revolution.com/}{Flash Flash Revolution} (FFR) is a renowned Flash-based vertical scroll rhythm game that was originally developed by \texttt{Synthlight} in 2002. Similar to its predecessor Dance Dance Revolution (DDR), players are presented with a sequence of arrows, referred to as \textit{stepfiles}, which are meticulously calibrated and synchronized to the background music. The main objective is to maximize the raw score by pressing the corresponding keys within a precise timing window of less than one-quarter of a second, ensuring a perfect alignment between the arrows in motion and the static receptors. Following the deprecation of Adobe Flash Player in 2020, FFR is now accessible on the Windows operating system through \href{https://www.flashflashrevolution.com/ffr/r3-goes-open-source/}{RCubed}, an open-source engine developed and actively maintained by \texttt{Velocity}.

\vspace{2mm}

The player base of FFR maintains a consistently robust presence, with several thousand users actively engaging in gameplay every month. Among this devoted community are competitive players hailing from online rhythm games such as \href{https://osu.ppy.sh/}{osu!mania} and \href{https://etternaonline.com/}{Etterna}, with a notable number participating in official tournaments each year. To ensure a positive experience for these players, many systems have been created in an attempt to objectively measure stepfile difficulty, fairly reward scores based on playing performance, and determine ratings that accurately reflect the player's skill. Although these systems are widely adopted in the community, their design has sparked considerable concerns regarding the reliability and scalability of their skill measurement.

\vspace{2mm}

Although examining different viewpoints is crucial, this report restricts its focus to a specific perspective and therefore operates under a set of assumptions in order to narrow down the scope of the problem. Justification for each assumption will be provided, but it is worth noting that the proposed solutions in this paper are not definitive and absolute. 
Previous discussions have shown that reaching a communal agreement is a needlessly difficult task, and there is limited cooperation within the community, as seen in \href{https://www.flashflashrevolution.com/vbz/showthread.php?t=153119}{prior debates} and \href{https://www.flashflashrevolution.com/vbz/showthread.php?t=149402}{disagreements}. It is suggested that developers tackle these issues by applying their own preferred assumptions, or by utilizing the analysis provided in this paper as a foundational starting point.

\vspace{2mm}

Additionally, unless specified otherwise, it should be noted that all figures presented in this paper are set to the downscroll setting, indicating that the arrows are presumed to move from the top of the screen to the bottom.

\subsection{Overview of FFR's Skill-Based Metrics and Their Shortcomings}
\subsubsection{Stepfile Difficulty}
Every stepfile possesses intrinsic levels of challenge concerning the player's execution due to their distinct structural characteristics. However, quantifying this measure continues to be a challenging question. In the initial stages of its development, stepfile difficulty was measured on an integer scale ranging from \texttt{1} to \texttt{12}, with \texttt{1} denoting the easiest and \texttt{12} denoting the most challenging levels in the game. In order to enhance difficulty assessment in response to the increasing volume of stepfiles in the engine, \texttt{nois-or-e}, \texttt{One Winged Angel}, and \texttt{stavie33} have restructured the difficulty scale to encompass a wider spectrum spanning from \texttt{1} to \texttt{120}. This update was \href{https://www.flashflashrevolution.com/vbz/showthread.php?t=124260}{announced on the forums} and has since been adopted as the current difficulty scale in use.

\vspace{2mm}

To assess the difficulty of new incoming stepfiles in the engine, a team of difficulty consultants within the community collaboratively assigns a numerical rating, considering factors such as input from other players and the level of difficulty presented by existing stepfiles rated at a similar level. After achieving a consensus, any necessary modifications are implemented in the game. Despite its reliability, this approach also raises certain concerns:

\begin{itemize}
	\item By solely relying on pairwise stepfile comparisons to determine the difficulty rating of an unrated stepfile, other assigned ratings are not considered during comparison, resulting in an inconsistent set of criteria established across stepfiles to define difficulty.
	\item Proposed difficulty ratings crowdsourced from the vocal minority are inherently subject to self-selection bias and group attribution error by the current process's mechanism design. Consequently, manual stepfile difficulty assignment for easier files is generally less accurate due to higher skill bias in the vocal minority.
    \item It is often challenging to compare two types of patterns in stepfiles due to the lack of a clear basis for drawing such comparisons. This often leads to prematurely brief discussions, in which users reach the incorrect conclusion that it is impossible to compare two stepfiles, despite the fact that this should not necessarily be the case.
\end{itemize}

Additionally, this current method of determining the difficulty of stepfiles is a manually daunting task, prompting many developers to seek algorithmic methods to measure stepfile difficulty automatically. With the growing advancements in machine learning and artificial intelligence, these methods have become increasingly popular for creating stepfile difficulty models. Recently in 2023, \texttt{Trumpet63} successfully developed an API incorporating a simple recurrent neural network model with a \href{https://www.flashflashrevolution.com/vbz/showthread.php?t=154255}{training mean-squared error of 10.975}, to help estimate the difficulty of new stepfiles in the \href{https://www.flashflashrevolution.com/~velocity/ffrjs/difficulty/}{Difficulty Estimator page}.

\vspace{2mm}

Although \texttt{Trumpet63}’s model has primarily received positive sentiment from the FFR community, these endeavors continue to face criticism, as there is a general lack of confidence in the model’s ability to accurately capture the multifaceted nature of this issue. This skepticism is further compounded by the model's inability to scale, which gives rise to the following concerns:

\begin{itemize}
	\item There are numerous interpretations of stepfile difficulty, making it challenging to establish a standardized definition that is universally accepted by the community.
	\item There is a subjective component inherent in the concept of difficulty that needs to be decoupled from the overall dataset to appropriately perform model training and inference.
	\item There is uncertainty on how meta stepfile patterns (e.g. jacks, jumpstream, etc.) can be appropriately defined and integrated as reliable features in model training. 
	\item There is an inability to 
	      predict difficulty at any rate not equal to \texttt{1.0x}, which has recently gained popularity for competitive play within the community.

       \item During model inference, the prediction latency required to estimate stepfile difficulty results in inadequate outcomes for both developers and end users due to high memory and storage costs.
\end{itemize}

Given the progress made in measuring stepfile difficulty and its widespread use as a means of facilitating discussions on player's proficiency, it is evident that this measure holds great significance in the FFR community. Therefore, it is imperative to establish a definitive and thorough understanding of stepfile difficulty, in order to mitigate any further challenges in defining other skill-based metrics.

\subsubsection{AAA Equivalency}

The \textit{AAA equivalency} was developed to determine the level of difficulty at which a player's performance on a specific stepfile can result in the highest attainable score (AAA). Particularly, this rating is determined by equating the player's performance on a stepfile under a given difficulty level to their potential ability to score a AAA on a stepfile under a different difficulty rating. The initial formula defining this metric was proposed by \texttt{Trumpet63} after conducting a \href{https://www.flashflashrevolution.com/vbz/showthread.php?p=4209451}{graphical analysis of player performance} during the 10\textsuperscript{th} Official FFR Tournament. However, as official tournaments have continued, the formula has been continuously updated and refined based on new data. These adjustments have resulted in the \href{https://www.flashflashrevolution.com/ffr/new-skill-based-ranking-system-leaderboards/}{current version of the formula}, which is outlined below.
\begin{align*}
	AAA_{eq} & = (D + \alpha) \left( \frac{\delta - NGC \cdot \lambda}{\delta}\right)^{1 / \beta} - \alpha  \\
	\\
	NGC      & = \text{Goods} + 1.8 \cdot \text{Averages} + 2.4 \cdot \text{Misses} + 0.2 \cdot \text{Boos} \\
	D        & = \text{Difficulty}                                                                          \\
	\\
	\delta   & = a_0 + a_1 \cdot D + a_2 \cdot D^2 + a_3 \cdot D^3 + a_4 \cdot D^4
\end{align*}
where
\begin{align*}
	a_0      & = 17678803623.9633                                                                           \\
	a_1      & = 733763392.922176                                                                           \\
	a_2      & = 28163834.4879901                                                                           \\
	a_3      & = -434698.513947563                                                                          \\
	a_4      & = 3060.24243867853
\end{align*}
and
\begin{align*}
	\lambda  & = 18206628.7286425                                                                           \\
	\alpha   & = 9.9750396740034                                                                            \\
	\beta    & = 0.0193296437339205                                                                         
\end{align*}

% \texttt{Matthias} showed that for all scores satisfying $NGC \leq 1049.2$, there exists a difficulty value $D$ ($1 \leq D \leq 120)$ such that $AAA_{eq} > 0$. This implies that scores with a raw good count exceeding 1050 may be disregarded from our analysis.

% \vspace{2mm}

Although the notion of AAA equivalency is widely accepted and acknowledged in the community, there are several concerns with its implementation. These concerns can be partly attributed to the lack of a consistent and accurate definition of stepfile difficulty.

\begin{itemize}
	\item Certain stepfiles have been identified to be more conducive to maximizing AAA equivalency, leading to unwarranted inflation when measuring the skill rating of a player.
	\item Players who attain impressive results with a score above the predefined threshold may receive inaccurate and undeserved ratings, particularly for many of the hardest stepfiles in game.
	\item The current AAA equivalency formula poses challenges in its interpretation, causing many individuals to question the rating they have received.
\end{itemize}

Various methods have been suggested on Discord to take into account the criticisms of the AAA equivalency measure. Proposed by \texttt{Lights}, a potential solution that shows promise suggests incorporating additional weighting in the formula to prevent players from exploiting the metric and inflate their skill ratings, while also accurately recognizing AAA equivalency for players who score above the current threshold of 28 raw goods on some of the most difficult stepfiles in FFR. However, these solutions are still in the process of being investigated and no significant progress has been made thus far.

\subsubsection{Skill Rating}

The \textit{Skill Rating} metric, which is determined by analyzing players' top-performing scores across their entire play record, serves as the primary basis for ranking players in FFR. Introduced by \texttt{Trumpet63}, \texttt{PrawnSkunk}, and \texttt{Silvuh}, this \href{https://www.flashflashrevolution.com/ffr/new-skill-based-ranking-system-leaderboards/}{ranking metric is initially defined} by computing an exponentially weighted average of a player's top $N$ scores. Particularly, the weighting mechanism that was rolled out in its initial release defines the weights $w := w(k, r)$ using the following formula for all ranks $r$ satisfying $r \in \{1, 2, \cdots, N\}$:
\begin{align*}
	w & = \frac{e^{k(r-1)} - e^{Nk}}{\displaystyle \sum_{j = 1}^{N} \left(e^{k(j-1)} - e^{Nk}\right)}
\end{align*}


% With this update, I would like to announce Trumpet63 as the newest member of the development team! His original “Zenith Score” analysis metric violently kickstarted this project, and his continued efforts in helping rework countless iterations of the skill rating formula were truly invaluable. Silvuh’s spreadsheet management and feedback during the development of “Zenith Score” sparked several innovative ideas and possibilities for the future of the rating system. s1rnight’s “GOAT Tracker” was a huge inspiration for this project, and her leaderboard logic formed the foundations of this project.

% And finally, thank you very much to the following volunteers for collaborating on the “Zenith Score” survey project during the 10th Official Tournament: Callipygian, LethalMutiny, noname219, PhantomPuppy, and Tim Allen.

where $k = - \frac{1}{4}$ and $N = 15$.

\vspace{2mm}

Under this configuration, note that:
\begin{align*}
	 \left. \sum_{r = 1}^5 w(k, r) \right|_{k = - \frac{1}{4}} & = \displaystyle \sum_{r = 1}^5 \frac{\exp\left(\displaystyle -\frac{r-1}{4}\right) - \exp\left(\displaystyle -\frac{15}{4}\right)}{\displaystyle\sum_{j = 1}^{15} \left[\exp\left(\displaystyle-\frac{j-1}{4}\right) - \exp\left(\displaystyle-\frac{15}{4}\right)\right]}\\
  &= \displaystyle \left( \displaystyle \frac{1 - \exp\left(\displaystyle-\frac{15}{4}\right)}{1 - \exp\left(\displaystyle-\frac{1}{4}\right)} - 15 \exp\left(\displaystyle-\frac{15}{4}\right) \right)^{-1}\left( \displaystyle \frac{1 - \exp\left(\displaystyle-\frac{5}{4}\right)}{1 - \exp\left(\displaystyle-\frac{1}{4}\right)} - 5 \exp\left(\displaystyle-\frac{15}{4}\right) \right)\\
  &\approx 0.76519.
\end{align*}

In other words, under this weighting, over 75\% of a player's skill rating is determined by the Top 5 scores. With the growing amount of stepfiles in FFR, there is an increase in the likelihood of a player obtaining a high score through chance, ultimately resulting in an incorrect representation of their true skill under this particular weighting mechanism. Moreover, relying on performance data indicative of a player's skill ceiling has been known to result in incorrect official tournament placements each year, thus creating an unfair disadvantage for certain players and diminishing their chances of winning.

\vspace{2mm}

To address some of these challenges, the  \href{https://www.flashflashrevolution.com/ffr/season-1/}{Seasonal Skill Rating} was implemented in conjunction with the established Skill Rating model in 2021. This system was specifically designed to monitor player performance in three-month intervals each year, providing more current data for use in determining official tournament placements. Developed by \texttt{Prawkskunk}, \texttt{XJ-9}, and \texttt{WirryWoo}, the weighting mechanism used in Seasonal Skill Rating calculates a linearly weighted average of a player's top $N$ scores using the formula provided below. For all ranks $r$ satisfying $r \in \{1, 2, \cdots, N\}$, the weight $w := w(r)$ is computed as follows:

\begin{align*}
	w & = \frac{N - r + 1}{\frac{1}{2}N (N + 1)} 
\end{align*}


For the initial implementation of the Seasonal Skill Rating, $N = 50$ was deliberately selected to evaluate its performance in comparison to the established Skill Rating metric. Under this weighting mechanism, we have:
\begin{align*}
	\sum_{r = 1}^5 w(r) & = \displaystyle \sum_{r = 1}^5 \frac{51 - r}{25 \cdot 51} \\
  &= \frac{1}{5} - \frac{15}{25 \cdot 51}\\
  &\approx 0.188235.
\end{align*}

This suggests that the player's skill rating is influenced by their Top 5 scores to a lesser extent, accounting for less than 20\% of their overall skill representation and significantly improving the first proposed weightings for skill rating. Based on favorable reviews following the first season's conclusion, the weighting mechanism of the Skill Rating model was revised to incorporate the new formula, which has since remained the primary approach for determining skill rating.
\vspace{2mm}

Despite the improvements, there are still several limitations in the Skill Rating model that necessitate further investigation: 
\begin{itemize}
	\item With the continuous addition of new stepfiles in FFR, players may have more opportunities to exploit $N$ particular stepfiles to achieve fluke scores. Conversely, for players with incomplete or unrepresentative score data, this may not accurately reflect their skill level. This raises the question of whether $N$ should be adjusted by the number of stepfiles played.
	\item The Seasonal Skill Rating only considers a player's top scores on stepfiles, but there is a need for a more comprehensive measure of all scores within a reasonable range from the player's Skill Rating, to improve tournament placements.
	      
\end{itemize}

Regrettably, due to insufficient availability of data, it is improbable that we will uncover resolutions for a number of the aforementioned issues. Nonetheless, these potential solutions will be outlined in the report to offer additional understanding on the necessary steps to tackle these problems.
\subsection{Modeling Disadvantages of Using Meta-Established Terminology}

In this section, we shift our focus to a contentious topic and explore the drawbacks of using meta stepfile features and raw goods in our machine learning models designed to improve the overall ranking system.

\subsubsection{Stepfile Patterns}

During gameplay, players observe similar patterns within numerous stepfiles and use these patterns to centralize discussions about strategies to optimally hit them. These patterns, referred to as \textit{meta stepfile features} in this report, are also currently used to draw comparisons between two different stepfiles to determine their level of difficulty. All identified patterns are depicted and animated in the \href{https://ffr.fandom.com/wiki/Category:Patterns}{Flash Flash Revolution Wiki}.

\vspace{2mm}

There are several challenges that arise when utilizing meta patterns to establish the difficulty of a stepfile. For instance, at very high rates, rolls compressed in a condensed time period are often treated as jumptrills. This technique, known as ``jumptrilling," is commonly used in competitive play to maximize scores in FFR. As this skill is easily accessible for many players, it is natural for them to equate it with the overall difficulty of the stepfile, potentially leading to a perception that the file is easier than initially thought.

\vspace{2mm}

On the contrary, the execution of jack patterns is commonly recognized as a highly individualized skill, with lower-ranked players often demonstrating greater proficiency in these patterns compared to their higher-ranked counterparts. Since the current ranking system rely heavily on stepfile difficulty, individuals with a one-dimensional skill set may potentially inflate their skill level and consequently, be inaccurately placed in official tournaments. Conflicting opinions on this matter, coupled with the inherent difficulty of executing jack patterns for most players, have resulted in varying perspectives among community members regarding the overall challenge posed by these patterns.

\vspace{2mm}

The provided examples demonstrate the absence of a unified agreement on the methodology for evaluating stepfile difficulty using meta stepfile patterns, which presents added challenges in developing a machine learning framework to predict stepfile difficulty. In order for such a model to be successful, these criteria must be accurately defined and widely accepted within the community. Unfortunately, the lack of agreement on this issue has impeded progress in this area and makes it unlikely that a resolution will be reached in the near future.

\vspace{2mm}

To address this issue, we suggest defining \textit{objective stepfile features} that conform to a widely accepted standard of difficulty, regardless of the player's level of skill and prior experience with the game. These features will be identified and further discussed in Section [TODO].

\subsubsection{Raw Goods}

\textit{Raw Goods} are designed to translate all imperfect judgments into a standardized score to better assess the player's performance on a given stepfile. This scoring was implemented as a replacement for combo scoring, as the latter was deemed to incentivize thoughtless and repetitive key pressing during gameplay. The point system for raw scoring is summarized below:

\begin{center}
	\begin{tabular}{c@{\hskip 10mm}c}
		\hspace{5mm} \textbf{Judgement} $j$ \hspace{5mm} & \textbf{Score} $s(j)$ \\
		\hline
		
		Perfect                               & 50                    \\
		Good                                  & 25                    \\
		Average                               & 5                     \\
		Miss                                  & $-$10                 \\
		Boo                                   & $-$5                  \\
	\end{tabular}
\end{center}

and raw goods $r(j)$ is computed as follows:

\begin{equation*}
	r(j) = 
	\begin{cases}
		\displaystyle \frac{s(j) - s(\text{Perfect})}{s(\text{Good}) - s(\text{Perfect})}
		  &   
		\text{if $j \neq \text{Boo}$;}
		\\[10pt]
		\displaystyle \frac{s(\text{Boo})}{s(\text{Good}) - s(\text{Perfect})}
		  &   
		\text{otherwise.}
	\end{cases}
\end{equation*}

This yields the following weights per note:

\begin{center}
	\begin{tabular}{c@{\hskip 5mm}c}
		\hspace{5mm} \textbf{Judgement} $j$ \hspace{5mm} & \textbf{Raw Goods} $r(j)$ \\
            \hline
		Perfect                               & 0                         \\
		Good                                  & 1                         \\
		Average                               & 1.8                       \\
		Miss                                  & 2.4                       \\
		Boo                                   & 0.2                       \\
	\end{tabular}
\end{center}

The aforementioned raw good values are subsequently aggregated according to the player's inputs against the notes present in the stepfile. These values, in conjunction with the stepfile difficulty, are currently used to quantify a player's performance using the AAA equivalency formula. Particularly, there exists an implicit, and potentially inaccurate, assumption that all notes possess equal significance in gauging a player's performance. Further research is necessary to determine the potential to improve player skill measurement by implementing a weighted representation system for each note, where weights are defined concerning its level of difficulty within a stepfile. 

\vspace{2mm}

Furthermore, the existing ranking system fails to account for differences in note counts when assessing players who achieve identical raw good counts on two stepfiles with the same difficulty rating but varying stepfile lengths. As a result, shortened stepfiles are viewed as comparatively easier to attain single-digit good (SDG) and AAA ratings on, prompting players to prioritize these files in order to increase their skill rating at a relatively minimal cost. As a result, there is a potential for discrepancies in performance evaluations between stepfiles, ultimately resulting in an unreliable determination of an individual's actual level of skill.

\vspace{2mm}

Based on the identified limitations of using raw goods, we will define \textit{score features} to improve accuracy in assessing the player's performance on the stepfile. This includes defining an adjusted variant of raw goods independent from note count and as previously mentioned, utilizing standard classification metrics from machine learning to measure performance based on the quantity of Misses and Boos. More details are to be covered in Section [TODO].

\subsection{Introduction to ACubed and Motivation of Research}

Following a discussion on the limitations of utilizing commonly accepted terminology within the FFR community and an overview of the deficiencies in FFR's current rating system, this paper presents ACubed, a machine learning framework to continuously train and evaluate models aimed to increase skill measurement scalability, while maintaining accuracy and minimizing required manual effort currently instilled in FFR's defined processes. The name ACubed has been chosen for the following reasons:

\begin{itemize}
	\item The competitive culture within the FFR community focuses heavily on AAA ratings, making the name of this new system particularly relevant.
	\item As introduced in Chapter \ref{sec:stepfile_difficulty}, the system being introduced will potentially be the third major update to FFR's difficulty system.
	\item ACubed is proposed as an enhancement to FFR's ranking system in spirit with RCubed being launched as a significant improvement to the game engine.
\end{itemize}

The summary of proposed changes is visually represented in the diagram shown in Figure \ref{fig1}. Particularly, the following changes are recommended:


\begin{itemize}
	\item The stepfile difficulty model will be isolated from player performance metrics such as skill rating and raw goods to minimize player subjectivity.
	\item Objective stepfile features will be utilized instead of relying on patterns established by the community to promote consistent terminology.
	\item Raw goods will be replaced with score features to more accurately track player performance.
	\item The AAA equivalency formula will no longer involve stepfile difficulty, but instead will directly incorporate objective stepfile features.
\end{itemize}
\begin{center}
	\begin{figure}[H]
		\centering
		\begin{subfloat}[Current State] {
				\centering
				\begin{tikzpicture}[->,>=stealth',auto,align=center,node distance=3cm,
					thick,main node/.style={font=\small}]
					
					\node[main node] (1) {Meta\\Stepfile\\Features};
					\node[main node] (2) [right=1.5cm] {Stepfile\\Difficulty};
					\node[main node] (3) [below right of=2] {Skill\\Rating};
					\node[main node] (4) [above right of=2] {Raw\\Goods};
					\node[main node] (5) [above right of=3] {AAA\\Equivalency};
					
					\path[every node/.style={font=\sffamily\small}]
					(1) edge node [left] {} (2)
					(1) edge [bend left] node [right] {} (4)
					(4) edge [bend right] node [right] {} (2)
					(3) edge [bend left] node [right] {} (4)
					(4) edge [bend left] node [right] {} (5)
					(2) edge [bend right] node [right] {} (5)
					(5) edge [bend left] node [right] {} (3)
					(3) edge [bend left] node [right] {} (2);
					    
				\end{tikzpicture}
			}
		\end{subfloat}
		\hspace{10pt}       
		\begin{subfloat}[Proposed State] {
				\centering
				\begin{tikzpicture}[->,>=stealth',auto,align=center,node distance=3cm,
					thick,main node/.style={font=\small}]
					
					\node[main node] (1) {Stepfile\\Difficulty};
					\node[main node] (2) [above=1.5cm] {Objective\\Stepfile\\Features};
					\node[main node] (3) [below right of=2] {Skill\\Rating};
					\node[main node] (4) [above right of=2] {Score\\Features};
					\node[main node] (5) [above right of=3] {AAA\\Equivalency};
					
					\path[every node/.style={font=\sffamily\small}]
					(2) edge node [left] {} (1)
					(2) edge [bend left] node [right] {} (4)
					(3) edge [bend left] node [right] {} (4)
					(4) edge [bend left] node [right] {} (5)
					(2) edge [bend right] node [right] {} (5)
					(5) edge [bend left] node [right] {} (3);
					    
				\end{tikzpicture}
			}       
		\end{subfloat}
		\caption{Dependency graphs differentiating the current and proposed states of skill-based FFR metrics.}
		\label{fig1}
		
	\end{figure}  
\end{center}

Additionally, it is important to note that the Proposed State, as illustrated in Figure \ref{fig1}b, exhibits a circular interdependence between Score Features, AAA Equivalency, and Skill Rating. A well-designed system should be able to:

\begin{itemize}
	\item Accurately assess a player's performance on a stepfile by utilizing score features derived from a predetermined skill rating metric.
	\item Calculate AAA Equivalency without making the assumption of uniform distribution of notes within the stepfile and also take into account performance metrics that are more focused on the player.
	\item Continuously adjust a player's skill rating without distorting their true skill level, which may be influenced by factors such as the number of stepfiles played.
\end{itemize}

The implementation of this study necessitates the development of a framework for training machine learning models using the expectation-maximization algorithm. Within this framework, the models are optimized concerning their corresponding dependent skill-based metrics based on a predetermined convergence criterion. Further details on this approach will be discussed in Section [TODO], with detailed steps for the technical implementation outlined in Section [TODO].

% \vspace{2mm}

% In Section \ref{sec:stepfile_difficulty}, we aim to establish a clear definition of stepfile difficulty by addressing the inherent subjectivity of this concept. We will examine various existing interpretations and propose a methodology for estimating difficulty based on these definitions. This process includes creating a data stratification strategy to incorporate player feedback from \texttt{\#difficulty-discussion} in FFR's Official Discord channel within our train-test split, identifying a set of objectively defined features that align with specific gameplay properties, and devising a framework for training and evaluating a time series extrinsic regression model. This model will be used to replicate experiments under varying hyperparameter configurations and considered definitions.

% \vspace{2mm}

% Our definition of AAA equivalency is consistent with the interpretation of the community. However, in Section [TODO], we propose improvements to the current formula by defining an adjusted raw good count concerning features identified for our stepfile difficulty model. Additionally, we introduce precision and recall as performance-indicative metrics within the realm of competitive gameplay by defining the ``Boo" judge accuracy label as false positives and the ``Miss" judge accuracy label as false negatives.

% \vspace{2mm}

% In Section [TODO], we propose a study advocating for the incorporation of outlier detection techniques to eliminate unrepresentative AAA equivalency scores. We also present a framework for adjusting the value of $N$ depending on the number of stepfiles played. Furthermore, we suggest incorporating Seasonal Skill Ratings to contextualize players' abilities beyond their top-performing scores and evaluate their performance in comparison to Skill Ratings.

\clearpage